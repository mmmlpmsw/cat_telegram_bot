{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweet-emotions-analysis-using-lstm-glove-roberta.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PsRWliTjKWrS",
        "YIRZ113PKWrT",
        "fgsGQOT_KWrX",
        "c-We5k6DKWrY",
        "LX5QJ7OFKWrZ",
        "aikSOsWLKWrb"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9YzG-H0KWrG"
      },
      "source": [
        "# Tweet Emotions Analysis (12 emotions) <a class=\"anchor\" id=\"tea\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVVqrWopKWrN",
        "outputId": "d547a9ae-4a7f-4c2c-86aa-b1e193338da6"
      },
      "source": [
        "!pip install emoji\n",
        "!pip install tweet-preprocessor 2>/dev/null 1>/dev/null\n",
        "!pip install kaggle\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñà                              | 10 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñâ                            | 20 kB 29.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 30 kB 12.9 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 40 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170 kB 5.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=3e75db470c4aee812a1e28da1f5a571a8d1f770babe5159faafc564505e1243d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.1\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.1 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 596 kB 43.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 895 kB 56.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59 kB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.3 MB 38.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adr9PkcXKWrO"
      },
      "source": [
        "import preprocessor as p\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import emoji\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers.recurrent import LSTM, GRU,SimpleRNN\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# from kaggle_datasets import KaggleDatasets\n",
        "import transformers\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsQW0lieKWrO"
      },
      "source": [
        "# Data preparation  <a class=\"anchor\" id=\"dp\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FgK77QJKWrP"
      },
      "source": [
        "#data = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\n",
        "#data2 = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CNu0WzZKWrP"
      },
      "source": [
        "data = pd.read_csv(\"text_emotion.csv\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlfBG1SKKWrP"
      },
      "source": [
        "### Misspelled data <a class=\"anchor\" id=\"dp-md\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvBr3QPTKWrQ",
        "outputId": "99960844-9418-42b5-97ac-34f96883baee"
      },
      "source": [
        "misspell_data = pd.read_csv(\"aspell.txt\",sep=\":\",names=[\"correction\",\"misspell\"])\n",
        "misspell_data.misspell = misspell_data.misspell.str.strip()\n",
        "misspell_data.misspell = misspell_data.misspell.str.split(\" \")\n",
        "misspell_data = misspell_data.explode(\"misspell\").reset_index(drop=True)\n",
        "misspell_data.drop_duplicates(\"misspell\",inplace=True)\n",
        "miss_corr = dict(zip(misspell_data.misspell, misspell_data.correction))\n",
        "\n",
        "#Sample of the dict\n",
        "{v:miss_corr[v] for v in [list(miss_corr.keys())[k] for k in range(20)]}"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Steffen': 'Stephen',\n",
              " 'abilitey': 'ability',\n",
              " 'abouy': 'about',\n",
              " 'absorbtion': 'absorption',\n",
              " 'accidently': 'accidentally',\n",
              " 'accomodate': 'accommodate',\n",
              " 'acommadate': 'accommodate',\n",
              " 'acord': 'accord',\n",
              " 'adultry': 'adultery',\n",
              " 'aggresive': 'aggressive',\n",
              " 'alchohol': 'alcohol',\n",
              " 'alchoholic': 'alcoholic',\n",
              " 'allieve': 'alive',\n",
              " 'alright': 'all_right',\n",
              " 'aquantance': 'acquaintance',\n",
              " 'equire': 'acquire',\n",
              " 'nevade': 'Nevada',\n",
              " 'presbyterian': 'Presbyterian',\n",
              " 'rsx': 'RSX',\n",
              " 'susan': 'Susan'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-4vAzOoKWrQ"
      },
      "source": [
        "def misspelled_correction(val):\n",
        "    for x in val.split(): \n",
        "        if x in miss_corr.keys(): \n",
        "            val = val.replace(x, miss_corr[x]) \n",
        "    return val\n",
        "\n",
        "data[\"clean_content\"] = data.content.apply(lambda x : misspelled_correction(x))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqB3esIhKWrQ"
      },
      "source": [
        "### Contractions <a class=\"anchor\" id=\"dp-c\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxXlD7IJKWrQ"
      },
      "source": [
        "contractions = pd.read_csv(\"contractions.csv\")\n",
        "cont_dic = dict(zip(contractions.Contraction, contractions.Meaning))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP3wbC3oKWrR"
      },
      "source": [
        "def cont_to_meaning(val): \n",
        "  \n",
        "    for x in val.split(): \n",
        "        if x in cont_dic.keys(): \n",
        "            val = val.replace(x, cont_dic[x]) \n",
        "    return val\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AldDQy0LKWrR"
      },
      "source": [
        "data.clean_content = data.clean_content.apply(lambda x : cont_to_meaning(x))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w54ZLuXKWrR"
      },
      "source": [
        "### Remove URLS and mentions <a class=\"anchor\" id=\"dp-r\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "4N_KhZa4KWrR",
        "outputId": "e43e396a-b3e1-4367-b7a8-d670623da8b5"
      },
      "source": [
        "p.set_options(p.OPT.MENTION, p.OPT.URL)\n",
        "p.clean(\"hello guys @alx #sportüî• 1245 https://github.com/s/preprocessor\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hello guys #sportüî• 1245'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3h8LVyZKWrR"
      },
      "source": [
        "data[\"clean_content\"]=data.content.apply(lambda x : p.clean(x))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsRWliTjKWrS"
      },
      "source": [
        "### Punctuations and emojis <a class=\"anchor\" id=\"dp-p\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XENENC-xKWrS"
      },
      "source": [
        "def punctuation(val): \n",
        "  \n",
        "    punctuations = '''()-[]{};:'\"\\,<>./@#$%^&_~'''\n",
        "  \n",
        "    for x in val.lower(): \n",
        "        if x in punctuations: \n",
        "            val = val.replace(x, \" \") \n",
        "    return val\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "9xbMZlPxKWrS",
        "outputId": "9f3e426e-2127-43a0-eaf9-9807d67f10ac"
      },
      "source": [
        "punctuation(\"test @ #ldfldlf??? !! \")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'test    ldfldlf??? !! '"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_Q_6WIiKWrS"
      },
      "source": [
        "data.clean_content = data.clean_content.apply(lambda x : ' '.join(punctuation(emoji.demojize(x)).split()))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV6Gbv-YKWrS"
      },
      "source": [
        "def clean_text(val):\n",
        "    val = misspelled_correction(val)\n",
        "    val = cont_to_meaning(val)\n",
        "    val = p.clean(val)\n",
        "    val = ' '.join(punctuation(emoji.demojize(val)).split())\n",
        "    \n",
        "    return val"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "EHwREGybKWrT",
        "outputId": "1f7ba997-8133-473b-da12-2e5819932347"
      },
      "source": [
        "clean_text(\"isn't üí° adultry @ttt good bad ... ! ? \")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'is not light bulb adultery good bad ! ?'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIRZ113PKWrT"
      },
      "source": [
        "### Remove empty comments <a class=\"anchor\" id=\"dp-re\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFchssCrKWrT"
      },
      "source": [
        "data = data[data.clean_content != \"\"]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5aHSgUhKWrT",
        "outputId": "5645e560-ed42-472d-89f9-f4e59c02baf5"
      },
      "source": [
        "data.sentiment.value_counts()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neutral       8579\n",
              "worry         8454\n",
              "happiness     5208\n",
              "sadness       5162\n",
              "love          3841\n",
              "surprise      2187\n",
              "fun           1776\n",
              "relief        1526\n",
              "hate          1323\n",
              "empty          815\n",
              "enthusiasm     759\n",
              "boredom        179\n",
              "anger          110\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-1Pmu83KWrU"
      },
      "source": [
        "# Modeling  <a class=\"anchor\" id=\"m\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UINf2WZMKWrU"
      },
      "source": [
        "### Encoding the data and train test split <a class=\"anchor\" id=\"m-ed\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO1riu1pKWrU"
      },
      "source": [
        "sent_to_id  = {\"empty\":0, \"sadness\":1,\"enthusiasm\":2,\"neutral\":3,\"worry\":4,\n",
        "                        \"surprise\":5,\"love\":6,\"fun\":7,\"hate\":8,\"happiness\":9,\"boredom\":10,\"relief\":11,\"anger\":12}"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCeX8WZmKWrU"
      },
      "source": [
        "data[\"sentiment_id\"] = data['sentiment'].map(sent_to_id)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "65g_job7KWrU",
        "outputId": "9ec2ce31-13b3-4e39-9269-8e032ccb6b3e"
      },
      "source": [
        "data"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>author</th>\n",
              "      <th>content</th>\n",
              "      <th>clean_content</th>\n",
              "      <th>sentiment_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1956967341</td>\n",
              "      <td>empty</td>\n",
              "      <td>xoshayzers</td>\n",
              "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
              "      <td>i know i was listenin to bad habit earlier and...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1956967666</td>\n",
              "      <td>sadness</td>\n",
              "      <td>wannamama</td>\n",
              "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
              "      <td>Layin n bed with a headache ughhhh waitin on y...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1956967696</td>\n",
              "      <td>sadness</td>\n",
              "      <td>coolfunky</td>\n",
              "      <td>Funeral ceremony...gloomy friday...</td>\n",
              "      <td>Funeral ceremony gloomy friday</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1956967789</td>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>czareaquino</td>\n",
              "      <td>wants to hang out with friends SOON!</td>\n",
              "      <td>wants to hang out with friends SOON!</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1956968416</td>\n",
              "      <td>neutral</td>\n",
              "      <td>xkilljoyx</td>\n",
              "      <td>@dannycastillo We want to trade with someone w...</td>\n",
              "      <td>We want to trade with someone who has Houston ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39994</th>\n",
              "      <td>1753918900</td>\n",
              "      <td>happiness</td>\n",
              "      <td>courtside101</td>\n",
              "      <td>Succesfully following Tayla!!</td>\n",
              "      <td>Succesfully following Tayla!!</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39996</th>\n",
              "      <td>1753919001</td>\n",
              "      <td>love</td>\n",
              "      <td>drapeaux</td>\n",
              "      <td>Happy Mothers Day  All my love</td>\n",
              "      <td>Happy Mothers Day All my love</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39997</th>\n",
              "      <td>1753919005</td>\n",
              "      <td>love</td>\n",
              "      <td>JenniRox</td>\n",
              "      <td>Happy Mother's Day to all the mommies out ther...</td>\n",
              "      <td>Happy Mother s Day to all the mommies out ther...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39998</th>\n",
              "      <td>1753919043</td>\n",
              "      <td>happiness</td>\n",
              "      <td>ipdaman1</td>\n",
              "      <td>@niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...</td>\n",
              "      <td>WASSUP BEAUTIFUL!!! FOLLOW ME!! PEEP OUT MY NE...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39999</th>\n",
              "      <td>1753919049</td>\n",
              "      <td>love</td>\n",
              "      <td>Alpharalpha</td>\n",
              "      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n",
              "      <td>bullet train from tokyo the gf and i have been...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>39919 rows √ó 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         tweet_id  ... sentiment_id\n",
              "0      1956967341  ...            0\n",
              "1      1956967666  ...            1\n",
              "2      1956967696  ...            1\n",
              "3      1956967789  ...            2\n",
              "4      1956968416  ...            3\n",
              "...           ...  ...          ...\n",
              "39994  1753918900  ...            9\n",
              "39996  1753919001  ...            6\n",
              "39997  1753919005  ...            6\n",
              "39998  1753919043  ...            9\n",
              "39999  1753919049  ...            6\n",
              "\n",
              "[39919 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfqhswdQKWrV"
      },
      "source": [
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(data.sentiment_id)\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "Y = onehot_encoder.fit_transform(integer_encoded)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v17FEt0KWrV"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data.clean_content,Y, random_state=1995, test_size=0.2, shuffle=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T52Z9i0eKWrV"
      },
      "source": [
        "### LSTM <a class=\"anchor\" id=\"m-l\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlujaxsIKWrV"
      },
      "source": [
        "# using keras tokenizer here\n",
        "token = text.Tokenizer(num_words=None)\n",
        "max_len = 160\n",
        "Epoch = 5\n",
        "token.fit_on_texts(list(X_train) + list(X_test))\n",
        "X_train_pad = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=max_len)\n",
        "X_test_pad = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=max_len)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mp4Sh4HKWrV"
      },
      "source": [
        "w_idx = token.word_index"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnpPmjwCKWrW",
        "outputId": "bbff4db3-cc75-440c-836b-d910f9538d43"
      },
      "source": [
        "embed_dim = 160\n",
        "lstm_out = 250\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(w_idx) +1 , embed_dim,input_length = X_test_pad.shape[1]))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(keras.layers.core.Dense(13, activation='softmax'))\n",
        "#adam rmsprop \n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 160, 160)          4835200   \n",
            "                                                                 \n",
            " spatial_dropout1d (SpatialD  (None, 160, 160)         0         \n",
            " ropout1D)                                                       \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 250)               411000    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 13)                3263      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,249,463\n",
            "Trainable params: 5,249,463\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAXW4SvXKWrW"
      },
      "source": [
        "batch_size = 32"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93IRBqI0KWrW",
        "outputId": "864c7c52-18fb-4162-e3a0-bf4cdb7e3317"
      },
      "source": [
        "model.fit(X_train_pad, y_train, epochs = Epoch, batch_size=batch_size,validation_data=(X_test_pad, y_test))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "998/998 [==============================] - 1476s 1s/step - loss: 2.0160 - accuracy: 0.3029 - val_loss: 1.8943 - val_accuracy: 0.3562\n",
            "Epoch 2/5\n",
            "998/998 [==============================] - 1482s 1s/step - loss: 1.7604 - accuracy: 0.4063 - val_loss: 1.8996 - val_accuracy: 0.3568\n",
            "Epoch 3/5\n",
            "998/998 [==============================] - 1509s 2s/step - loss: 1.4924 - accuracy: 0.5050 - val_loss: 2.0407 - val_accuracy: 0.3393\n",
            "Epoch 4/5\n",
            "998/998 [==============================] - 1511s 2s/step - loss: 1.1977 - accuracy: 0.6100 - val_loss: 2.2653 - val_accuracy: 0.3066\n",
            "Epoch 5/5\n",
            "998/998 [==============================] - 1491s 1s/step - loss: 0.9455 - accuracy: 0.6913 - val_loss: 2.5191 - val_accuracy: 0.2970\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3670b9d3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aOp1UoTIhui",
        "outputId": "e92aa20c-ee3d-48f0-cab8-59dc044d2cd4"
      },
      "source": [
        "model.save('train_model.pb')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: train_model.pb/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:<keras.layers.recurrent.LSTM object at 0x7f3670fa9f90> has the same name 'LSTM' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTM'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f3670fbf950> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LHTqq28QPxH",
        "outputId": "052a96d8-cabd-4440-8abe-ef02daae45fc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7n28ovEKWrW"
      },
      "source": [
        "def get_sentiment(model,text):\n",
        "    text = clean_text(text)\n",
        "    #tokenize\n",
        "    twt = token.texts_to_sequences([text])\n",
        "    twt = sequence.pad_sequences(twt, maxlen=max_len, dtype='int32')\n",
        "    sentiment = model.predict(twt,batch_size=1,verbose = 2)\n",
        "    sent = np.round(np.dot(sentiment,100).tolist(),0)[0]\n",
        "    result = pd.DataFrame([sent_to_id.keys(),sent]).T\n",
        "    result.columns = [\"sentiment\",\"percentage\"]\n",
        "    result=result[result.percentage !=0]\n",
        "    return result"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcxEHxIAKWrW"
      },
      "source": [
        "def plot_result(df):\n",
        "    #colors=['#D50000','#000000','#008EF8','#F5B27B','#EDECEC','#D84A09','#019BBD','#FFD000','#7800A0','#098F45','#807C7C','#85DDE9','#F55E10']\n",
        "    #fig = go.Figure(data=[go.Pie(labels=df.sentiment,values=df.percentage, hole=.3,textinfo='percent',hoverinfo='percent+label',marker=dict(colors=colors, line=dict(color='#000000', width=2)))])\n",
        "    #fig.show()\n",
        "    colors={'love':'rgb(213,0,0)','empty':'rgb(0,0,0)',\n",
        "                    'sadness':'rgb(0,142,248)','enthusiasm':'rgb(245,178,123)',\n",
        "                    'neutral':'rgb(237,236,236)','worry':'rgb(216,74,9)',\n",
        "                    'surprise':'rgb(1,155,189)','fun':'rgb(255,208,0)',\n",
        "                    'hate':'rgb(120,0,160)','happiness':'rgb(9,143,69)',\n",
        "                    'boredom':'rgb(128,124,124)','relief':'rgb(133,221,233)',\n",
        "                    'anger':'rgb(245,94,16)'}\n",
        "    col_2={}\n",
        "    for i in result.sentiment.to_list():\n",
        "        col_2[i]=colors[i]\n",
        "    fig = px.pie(df, values='percentage', names='sentiment',color='sentiment',color_discrete_map=col_2,hole=0.3)\n",
        "    fig.show()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF96gXtTKWrX"
      },
      "source": [
        "### Test LSTM Results <a class=\"anchor\" id=\"m-lr\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbu47QYIKWrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d591da5f-26b8-4cd4-c3a8-611a32624bda"
      },
      "source": [
        "result =get_sentiment(model,\"Had an absolutely brilliant day √∞≈∏Àú¬Å loved seeing an old friend and reminiscing\")\n",
        "# plot_result(result)\n",
        "print(result)\n",
        "result =get_sentiment(model,\"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I can‚Äôt hold myself back. I really miss you\")\n",
        "# plot_result(result)\n",
        "result =get_sentiment(model,\"I hate this game so much,It make me angry all the time \")\n",
        "# plot_result(result)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 - 0s - 100ms/epoch - 100ms/step\n",
            "     sentiment percentage\n",
            "0        empty          1\n",
            "1      sadness          4\n",
            "2   enthusiasm          1\n",
            "3      neutral          3\n",
            "4        worry          1\n",
            "5     surprise          1\n",
            "6         love         19\n",
            "7          fun         33\n",
            "9    happiness         35\n",
            "11      relief          2\n",
            "1/1 - 0s - 102ms/epoch - 102ms/step\n",
            "1/1 - 0s - 95ms/epoch - 95ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgsGQOT_KWrX"
      },
      "source": [
        "### LSTM with glove 6B 200d word embedding <a class=\"anchor\" id=\"m-lg\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhFspiNoKWrX"
      },
      "source": [
        "def read_data(file_name):\n",
        "    with open(file_name,'r') as f:\n",
        "        word_vocab = set() \n",
        "        word2vector = {}\n",
        "        for line in f:\n",
        "            line_ = line.strip() \n",
        "            words_Vec = line_.split()\n",
        "            word_vocab.add(words_Vec[0])\n",
        "            word2vector[words_Vec[0]] = np.array(words_Vec[1:],dtype=float)\n",
        "    print(\"Total Words in DataSet:\",len(word_vocab))\n",
        "    return word_vocab,word2vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfNmM28kKWrX"
      },
      "source": [
        "vocab, word_to_idx =read_data(\"/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz9J0-BjKWrY"
      },
      "source": [
        "embedding_matrix = np.zeros((len(w_idx) + 1, 200))\n",
        "for word, i in w_idx.items():\n",
        "    embedding_vector = word_to_idx.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQYc_oP4KWrY"
      },
      "source": [
        "embed_dim = 200\n",
        "lstm_out = 250\n",
        "\n",
        "model_lstm_gwe = Sequential()\n",
        "model_lstm_gwe.add(Embedding(len(w_idx) +1 , embed_dim,input_length = X_test_pad.shape[1],weights=[embedding_matrix],trainable=False))\n",
        "model_lstm_gwe.add(SpatialDropout1D(0.2))\n",
        "model_lstm_gwe.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model_lstm_gwe.add(keras.layers.core.Dense(13, activation='softmax'))\n",
        "#adam rmsprop \n",
        "model_lstm_gwe.compile(loss = \"categorical_crossentropy\", optimizer='adam',metrics = ['accuracy'])\n",
        "print(model_lstm_gwe.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feaEa7pdKWrY"
      },
      "source": [
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMc0iP0RKWrY"
      },
      "source": [
        "\n",
        "model_lstm_gwe.fit(X_train_pad, y_train, epochs = Epoch, batch_size=batch_size,validation_data=(X_test_pad, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-We5k6DKWrY"
      },
      "source": [
        "### Test LSTM glove Results <a class=\"anchor\" id=\"m-lgr\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGdLpi7nKWrZ"
      },
      "source": [
        "result =get_sentiment(model_lstm_gwe,\"Had an absolutely brilliant day √∞≈∏Àú¬Å loved seeing an old friend and reminiscing\")\n",
        "plot_result(result)\n",
        "result =get_sentiment(model_lstm_gwe,\"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I can‚Äôt hold myself back. I really miss you\")\n",
        "plot_result(result)\n",
        "result =get_sentiment(model_lstm_gwe,\"I hate this game so much,It make me angry all the time \")\n",
        "plot_result(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX5QJ7OFKWrZ"
      },
      "source": [
        "### Roberta Base Model <a class=\"anchor\" id=\"m-rb\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERUgL_1bKWrZ"
      },
      "source": [
        "def regular_encode(texts, tokenizer, maxlen=512):\n",
        "    enc_di = tokenizer.batch_encode_plus(\n",
        "        texts, \n",
        "        return_attention_masks=False, \n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=maxlen\n",
        "    )\n",
        "    \n",
        "    return np.array(enc_di['input_ids'])\n",
        "\n",
        "def build_model(transformer, max_len=160):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "    out = Dense(13, activation='softmax')(cls_token)\n",
        "    \n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnH-esq5KWrZ"
      },
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "MODEL = 'roberta-base'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdWySc0fKWrZ"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_le3K3xjKWra"
      },
      "source": [
        "X_train_t = regular_encode(X_train, tokenizer, maxlen=max_len)\n",
        "X_test_t = regular_encode(X_test, tokenizer, maxlen=max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jafiAqRNKWra"
      },
      "source": [
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((X_train_t, y_train))\n",
        "    .repeat()\n",
        "    .shuffle(1995)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((X_test_t, y_test))\n",
        "    .batch(batch_size)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H8pmMo1KWra"
      },
      "source": [
        "transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
        "model_roberta_base = build_model(transformer_layer, max_len=max_len)\n",
        "model_roberta_base.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dns2n7YKWra"
      },
      "source": [
        "n_steps = X_train.shape[0] // batch_size\n",
        "model_roberta_base.fit(train_dataset,steps_per_epoch=n_steps,validation_data=valid_dataset,epochs=Epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aikSOsWLKWrb"
      },
      "source": [
        "### Test Roberta Model Results <a class=\"anchor\" id=\"m-rbr\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l9KM6FVKWrb"
      },
      "source": [
        "def get_sentiment2(model,text):\n",
        "    text = clean_text(text)\n",
        "    #tokenize\n",
        "    x_test1 = regular_encode([text], tokenizer, maxlen=max_len)\n",
        "    test1 = (tf.data.Dataset.from_tensor_slices(x_test1).batch(1))\n",
        "    #test1\n",
        "    sentiment = model.predict(test1,verbose = 0)\n",
        "    sent = np.round(np.dot(sentiment,100).tolist(),0)[0]\n",
        "    result = pd.DataFrame([sent_to_id.keys(),sent]).T\n",
        "    result.columns = [\"sentiment\",\"percentage\"]\n",
        "    result=result[result.percentage !=0]\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mcNccUhKWrb"
      },
      "source": [
        "result =get_sentiment2(model_roberta_base,\"Had an absolutely brilliant day √∞≈∏Àú¬Å loved seeing an old friend and reminiscing\")\n",
        "plot_result(result)\n",
        "result =get_sentiment2(model_roberta_base,\"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I can‚Äôt hold myself back. I really miss you\")\n",
        "plot_result(result)\n",
        "result =get_sentiment2(model_roberta_base,\"I hate this game so much,It make me angry all the time \")\n",
        "plot_result(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHQ-E9JXKWrb"
      },
      "source": [
        "# Albert Base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Fb-Plx9KWrb"
      },
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "MODEL = 'albert-base-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "X_train_t = regular_encode(X_train, tokenizer, maxlen=max_len)\n",
        "X_test_t = regular_encode(X_test, tokenizer, maxlen=max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHkssrBPKWrb"
      },
      "source": [
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((X_train_t, y_train))\n",
        "    .repeat()\n",
        "    .shuffle(1995)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((X_test_t, y_test))\n",
        "    .batch(batch_size)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZin1zkQKWrc"
      },
      "source": [
        "transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
        "albert = build_model(transformer_layer, max_len=max_len)\n",
        "albert.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG4aP8o9KWrc"
      },
      "source": [
        "n_steps = X_train.shape[0] // batch_size\n",
        "albert.fit(train_dataset,steps_per_epoch=n_steps,validation_data=valid_dataset,epochs=Epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-Bonau8KWrc"
      },
      "source": [
        "result =get_sentiment2(albert,\"Had an absolutely brilliant day √∞≈∏Àú¬Å loved seeing an old friend and reminiscing\")\n",
        "plot_result(result)\n",
        "result =get_sentiment2(albert,\"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I can‚Äôt hold myself back. I really miss you\")\n",
        "plot_result(result)\n",
        "result =get_sentiment2(albert,\"I hate this game so much,It make me angry all the time \")\n",
        "plot_result(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}